{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7HQNng1CR0xY",
   "metadata": {
    "id": "7HQNng1CR0xY"
   },
   "source": [
    "# **Initial tuning**\n",
    "\n",
    "Initial tuning of model parameters (hidden size, number of layers and attention heads) to get a basic model to start with and use for future experiments. These parameter values may be changed before the final training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a1c8f",
   "metadata": {
    "id": "762a1c8f"
   },
   "source": [
    "### **Import data, modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k4h4FVAojU1I",
   "metadata": {
    "id": "k4h4FVAojU1I"
   },
   "outputs": [],
   "source": [
    "# Import all modules and functions\n",
    "import scripts.functions as functions\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86825275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T12:43:33.970935Z",
     "start_time": "2025-10-12T12:43:33.720666Z"
    },
    "id": "86825275"
   },
   "outputs": [],
   "source": [
    "# load the train and test data sets\n",
    "with open(\"text8_train.txt\", \"r\") as f:\n",
    "    train_text = f.read()\n",
    "with open(\"text8_test.txt\", \"r\") as f:\n",
    "    test_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdca63",
   "metadata": {
    "id": "7bfdca63"
   },
   "source": [
    "Build vocabulary (lowercase + space + a few punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1eafe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T12:43:40.137905Z",
     "start_time": "2025-10-12T12:43:40.111851Z"
    },
    "id": "34b1eafe"
   },
   "outputs": [],
   "source": [
    "# Define a character list with length 27\n",
    "char_set = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
    "# Encode each letter to an integer and vice versa\n",
    "char_to_int = {ch:i for i,ch in enumerate(char_set)}\n",
    "int_to_char = {i:ch for ch,i in char_to_int.items()}\n",
    "\n",
    "def encode(s):\n",
    "    ids = [char_to_int[c] for c in s]\n",
    "    return np.array(ids, dtype=np.uint8)  # use np.uint8 to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k0w_Nlx0Rp32",
   "metadata": {
    "id": "k0w_Nlx0Rp32"
   },
   "outputs": [],
   "source": [
    "# Encode the text into integers\n",
    "train_text_int = encode(train_text)\n",
    "test_text_int = encode(test_text)\n",
    "\n",
    "# Save the data\n",
    "np.save(\"train_text_int.npy\", train_text_int)\n",
    "np.save(\"test_text_int.npy\", test_text_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724c34b",
   "metadata": {
    "id": "7724c34b"
   },
   "source": [
    "# **Create basic Transformer model**\n",
    "\n",
    "Initialize the basic model architecture and parameters.\n",
    "In Flax, model parameters are stored as a nested PyTree, similar to nested dictionaries/lists of arrays.\n",
    "\n",
    "**Disclaimer**: Adjusting the model architecture can affect the number of model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GNBReZyUiNIw",
   "metadata": {
    "id": "GNBReZyUiNIw"
   },
   "outputs": [],
   "source": [
    "# Import model\n",
    "import models.basic_transformer as basic_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5955c3f",
   "metadata": {
    "id": "f5955c3f"
   },
   "outputs": [],
   "source": [
    "# Function to initialize basic Transformer model and its params\n",
    "def create_train_state(key, vocab_size, d_model, n_layers, n_heads, max_len):\n",
    "    model = basic_transformer.DecoderOnlyTransformer(vocab_size, d_model, n_layers, n_heads, max_len)\n",
    "\n",
    "    # Create dummy input for initialization of batch size 1, seq length min(16, max_len)\n",
    "    dummy = jnp.zeros((1, min(16, max_len)), dtype=jnp.int32)\n",
    "    # Initialize the parameters and extracts the PyTree of params\n",
    "    params = model.init({\"params\": key}, dummy)[\"params\"]\n",
    "    return model, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea2291",
   "metadata": {
    "id": "c3ea2291"
   },
   "outputs": [],
   "source": [
    "# FIXED vocab size\n",
    "vocab_size=len(char_set)\n",
    "\n",
    "# maximum sequence length\n",
    "max_len=128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05221147",
   "metadata": {
    "id": "05221147"
   },
   "source": [
    "### **Optimization step**\n",
    "Performs a single gradient descent update. Updates the parameters and optimizer state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720c846",
   "metadata": {
    "id": "b720c846"
   },
   "outputs": [],
   "source": [
    "# gradient update\n",
    "def train_step(params, opt_state, x, y, tx):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      params: pytree of model parameters.\n",
    "      opt_state: optax optimizer state corresponding to `params`.\n",
    "      x: (B_seq, B_tok) int array input tokens.\n",
    "      y: (B_seq, B_tok) int array target tokens.\n",
    "      tx: optax.GradientTransformation (already initialized).\n",
    "\n",
    "    Returns:\n",
    "      new_params: updated parameters after one gradient step.\n",
    "      new_opt_state: updated optimizer state.\n",
    "      metrics: dict of scalar metrics (loss, acc).\n",
    "    \"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits = model.apply({\"params\": params}, x)\n",
    "        loss, metrics = loss_and_metrics(logits, y)\n",
    "        return loss, metrics\n",
    "\n",
    "    # compute gradients of loss w.r.t params (loss is scalar, metrics is auxiliary)\n",
    "    (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
    "\n",
    "    # optax update: update params and new optimizer state\n",
    "    updates, new_opt_state = tx.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state, metrics\n",
    "\n",
    "# jit: last argument should be static because it is an object\n",
    "train_step = jax.jit(train_step, static_argnames=(\"tx\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4plFrTTVhdHw",
   "metadata": {
    "id": "4plFrTTVhdHw"
   },
   "source": [
    "In this section, the architectural parameters that will be tuned are:\n",
    "- Hidden size\n",
    "- Number of layers\n",
    "- Number of attention heads\n",
    "\n",
    "**Objective**: To simply find a combination of model parameters that best improve model performance (the validation loss is the metric for this). Training time is also taken into account.\n",
    "\n",
    "With GPU memory usage in mind, a reduced parameter grid is used:\n",
    "- For hidden size, it will be fixed at 256. In future experiments, nearer to the end of the project, there will be an experiment on increasing hidden size.\n",
    "- The number of attention heads does not affect the number of model parameters. Less restriction is exercised here.\n",
    "- If there are more layers, it contributes to parameter count. It is mindful to settle on a value to the left of 8.\n",
    "\n",
    "Batch size and number of iterations are reduced for a faster runtime.\n",
    "\n",
    "**Disclaimer**: Transformer fine-tuning is very resource-intensive, especially because of the number of model parameters. With this in mind, constraints are exercised for all future experiments to avoid hitting the GPU usage limit on Colab. Furthermore, this experiment is to merely find an initial transformer model to use for future small-scale experiments. Parameter values may or may not be the same during the final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mt65Y8hEFqoZ",
   "metadata": {
    "id": "mt65Y8hEFqoZ"
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"d_model\": [256],\n",
    "    \"n_heads\": [8, 16],\n",
    "    \"n_layers\": [4, 8]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FEP2BZXOKoIh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1761199281465,
     "user": {
      "displayName": "Joshua Ng",
      "userId": "12368878847525278040"
     },
     "user_tz": -480
    },
    "id": "FEP2BZXOKoIh",
    "outputId": "b9da3d12-d871-46b9-b0ae-6dadf8c9e81d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000000\n",
      "10000000\n",
      "80000000\n"
     ]
    }
   ],
   "source": [
    "# For model tuning, separate train_text_int into training and validation sets\n",
    "print(len(train_text_int))\n",
    "\n",
    "# 1/9 of the training set will be used for validation\n",
    "val_text_int = train_text_int[-10_000_000:]\n",
    "training_text_int = train_text_int[:80_000_000]\n",
    "print(len(val_text_int))\n",
    "print(len(training_text_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reXczUnHw8Ci",
   "metadata": {
    "id": "reXczUnHw8Ci"
   },
   "source": [
    "### **Early Stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MQflnrXavdpK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 432567,
     "status": "ok",
     "timestamp": 1761199863489,
     "user": {
      "displayName": "Joshua Ng",
      "userId": "12368878847525278040"
     },
     "user_tz": -480
    },
    "id": "MQflnrXavdpK",
    "outputId": "10224292-83b2-401f-9358-03b8ad1d7a3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer model initialized with a hidden size of 256, 4 layers and 8 heads\n",
      "Early stopping implemented at iteration 4000\n",
      "Transformer model initialized with a hidden size of 256, 8 layers and 8 heads\n",
      "Early stopping implemented at iteration 4000\n",
      "Transformer model initialized with a hidden size of 256, 4 layers and 16 heads\n",
      "Early stopping implemented at iteration 4000\n",
      "Transformer model initialized with a hidden size of 256, 8 layers and 16 heads\n",
      "Early stopping implemented at iteration 4000\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "min_improvement = 1e-3   # Considering computation of validation loss periodically\n",
    "new_params = None\n",
    "validation_losses_of_all = []\n",
    "times_of_all = []\n",
    "models = []\n",
    "\n",
    "for d_model, n_heads, n_layers in product(param_grid[\"d_model\"], param_grid[\"n_heads\"], param_grid[\"n_layers\"]):\n",
    "    new_params = [d_model, n_heads, n_layers]\n",
    "    models.append(new_params)\n",
    "    # Initialize the model architecture and params\n",
    "    model, params = create_train_state(key, vocab_size, d_model, n_layers, n_heads, max_len)\n",
    "    print(f\"Transformer model initialized with a hidden size of {d_model}, {n_layers} layers and {n_heads} heads\")\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    tx = optax.adam(learning_rate=learning_rate)\n",
    "    # Initialize optimizer state for current params, learning rate is 0.001 for gradient descent\n",
    "    opt_state = tx.init(params)\n",
    "\n",
    "    niter = 20_000                    # reduced no of iterations for each model\n",
    "    B_seq, B_tok = 64, 32             # sample a smaller no of sequences per batch, same no of tokens per seq\n",
    "    loss_test_history = []\n",
    "\n",
    "    time_start = time.time()\n",
    "    patience_counter = 0\n",
    "    fully_trained = False\n",
    "    # Keep track of the best_val_loss for each model\n",
    "    best_val_loss = 1000\n",
    "    for it in range(niter):\n",
    "        batch = get_batch(training_text_int, B_seq, B_tok)\n",
    "        input, target = batch[0], batch[1]\n",
    "        params_new, opt_state_new, metrics = train_step(params, opt_state, input, target, tx)\n",
    "\n",
    "        # update model weights and optimizer state\n",
    "        params = params_new\n",
    "        opt_state = opt_state_new\n",
    "\n",
    "        if it == niter - 1:\n",
    "            fully_trained = True\n",
    "\n",
    "\n",
    "        # Implement early stopping; compute validation loss periodically\n",
    "        if (it%1000 == 0) or fully_trained:\n",
    "            new_time = time.time() - time_start\n",
    "            B_val, T_val = 1024, 32\n",
    "            val_batch = get_batch(val_text_int, B_val, T_val)\n",
    "            val_input, val_target = val_batch[0], val_batch[1]\n",
    "            val_logits = model.apply({\"params\": params}, val_input)\n",
    "            val_loss, val_metrics = loss_and_metrics(val_logits, val_target)\n",
    "\n",
    "            if val_loss + min_improvement < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter += 1\n",
    "\n",
    "                if patience_counter >= 5:\n",
    "                    print(f\"Early stopping implemented at iteration {it}\")\n",
    "                    # If early stopping, break and keep tabs on latest training time and validation loss\n",
    "                    validation_losses_of_all.append(val_loss)\n",
    "                    times_of_all.append(new_time)\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # If the training goes all the way without early stopping, keep tabs on total training time and latest validation loss\n",
    "            elif fully_trained:\n",
    "                validation_losses_of_all.append(val_loss)\n",
    "                times_of_all.append(new_time)\n",
    "\n",
    "# number of permutations of model parameters, there should only be 4\n",
    "print(len(param_grid[\"d_model\"]) * len(param_grid[\"n_heads\"]) * len(param_grid[\"n_layers\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kQXOApaTw_zh",
   "metadata": {
    "id": "kQXOApaTw_zh"
   },
   "source": [
    "### **Full model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4wiuSoDUHbIj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2034275,
     "status": "ok",
     "timestamp": 1761203414138,
     "user": {
      "displayName": "Joshua Ng",
      "userId": "12368878847525278040"
     },
     "user_tz": -480
    },
    "id": "4wiuSoDUHbIj",
    "outputId": "c980d65a-bd5c-49d8-8eba-1a4ba727645d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer model initialized with a hidden size of 256, 4 layers and 8 heads\n",
      "Transformer model initialized with a hidden size of 256, 8 layers and 8 heads\n",
      "Transformer model initialized with a hidden size of 256, 4 layers and 16 heads\n",
      "Transformer model initialized with a hidden size of 256, 8 layers and 16 heads\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "new_params = None\n",
    "full_validation_losses_of_all = []\n",
    "full_times_of_all = []\n",
    "models = []\n",
    "\n",
    "for d_model, n_heads, n_layers in product(param_grid[\"d_model\"], param_grid[\"n_heads\"], param_grid[\"n_layers\"]):\n",
    "    new_params = [d_model, n_heads, n_layers]\n",
    "    models.append(new_params)\n",
    "    # Initialize the model architecture and params\n",
    "    model, params = create_train_state(key, vocab_size, d_model, n_layers, n_heads, max_len)\n",
    "    print(f\"Transformer model initialized with a hidden size of {d_model}, {n_layers} layers and {n_heads} heads\")\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    tx = optax.adam(learning_rate=learning_rate)\n",
    "    # Initialize optimizer state for current params\n",
    "    opt_state = tx.init(params)\n",
    "\n",
    "    niter = 20_000                    # reduced no of iterations for each model\n",
    "    B_seq, B_tok = 64, 32             # sample a smaller no of sequences per batch, same no of tokens per seq\n",
    "\n",
    "    time_start = time.time()\n",
    "    # Keep track of the best_val_loss for each model\n",
    "    best_val_loss = 1000\n",
    "    for it in range(niter):\n",
    "        batch = get_batch(training_text_int, B_seq, B_tok)\n",
    "        input, target = batch[0], batch[1]\n",
    "        params_new, opt_state_new, metrics = train_step(params, opt_state, input, target, tx)\n",
    "\n",
    "        # update model weights and optimizer state\n",
    "        params = params_new\n",
    "        opt_state = opt_state_new\n",
    "\n",
    "        # If the last iteration\n",
    "        if it == niter - 1:\n",
    "            new_time = time.time() - time_start\n",
    "            B_val, T_val = 1024, 32\n",
    "            val_batch = get_batch(val_text_int, B_val, T_val)\n",
    "            val_input, val_target = val_batch[0], val_batch[1]\n",
    "            val_logits = model.apply({\"params\": params}, val_input)\n",
    "            val_loss, val_metrics = loss_and_metrics(val_logits, val_target)\n",
    "            full_validation_losses_of_all.append(val_loss)\n",
    "            full_times_of_all.append(new_time)\n",
    "\n",
    "\n",
    "# number of permutations of model parameters, there should only be 4\n",
    "print(len(param_grid[\"d_model\"]) * len(param_grid[\"n_heads\"]) * len(param_grid[\"n_layers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P3yeX--oH_sY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1761199983787,
     "user": {
      "displayName": "Joshua Ng",
      "userId": "12368878847525278040"
     },
     "user_tz": -480
    },
    "id": "P3yeX--oH_sY",
    "outputId": "36ce5e5f-d23a-45d2-8c54-e6d7b78174c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model with parameters [256, 8, 4] has a training time of 70.73276329040527 and a validation loss of 1.4581079483032227\n",
      "This model with parameters [256, 8, 8] has a training time of 137.31607007980347 and a validation loss of 1.4512369632720947\n",
      "This model with parameters [256, 16, 4] has a training time of 75.04076051712036 and a validation loss of 1.4715590476989746\n",
      "This model with parameters [256, 16, 8] has a training time of 146.54027605056763 and a validation loss of 1.467713713645935\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with early stopping at the 4_000th iteration\n",
    "for i in range(4):\n",
    "    print(f\"This model with parameters {models[i]} has a training time of {times_of_all[i]} and a validation loss of {validation_losses_of_all[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QbXVz4aw0d_R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1761203482884,
     "user": {
      "displayName": "Joshua Ng",
      "userId": "12368878847525278040"
     },
     "user_tz": -480
    },
    "id": "QbXVz4aw0d_R",
    "outputId": "5759462e-243a-4059-d3d8-f63272b5eb6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model with parameters [256, 8, 4] has a total training time of 331.1321680545807 and a final validation loss of 1.3307313919067383\n",
      "This model with parameters [256, 8, 8] has a total training time of 652.1351611614227 and a final validation loss of 1.3208262920379639\n",
      "This model with parameters [256, 16, 4] has a total training time of 351.33376145362854 and a final validation loss of 1.3433935642242432\n",
      "This model with parameters [256, 16, 8] has a total training time of 696.6427783966064 and a final validation loss of 1.3371410369873047\n"
     ]
    }
   ],
   "source": [
    "# Evaluation for full training\n",
    "for i in range(4):\n",
    "    print(f\"This model with parameters {models[i]} has a total training time of {full_times_of_all[i]} and a final validation loss of {full_validation_losses_of_all[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zp8207KNs3pv",
   "metadata": {
    "id": "Zp8207KNs3pv"
   },
   "source": [
    "### **Evaluation**\n",
    "\n",
    "**Decision**: Model with a **hidden size of 256, 4 layers and 8 heads**.\n",
    "\n",
    "1. First result using early stopping (to get an initial sense of model performance)\n",
    "- For the models that have 8 heads compared to 16 heads, model performance is slightly better.\n",
    "- Between [256, 8, 4] and [256, 8, 8], as the number of layers doubled, the training time nearly doubled while model performance very slightly improves.\n",
    "\n",
    "2. Second result (full model training)\n",
    "- These results reassert the decision because by comparing the two combination of parameters that have 8 heads, model performance improves by a very small margin despite training time doubling. There are many other ways to optimize the model other than increase the number of layers."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
